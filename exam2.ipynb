{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, cohen_kappa_score,mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from pickle import dump,load\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "\n",
    "\n",
    "def cohen_kappa_scorer(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return cohen_kappa_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "\n",
    "* Freezing (precipitation + temp < 32)\n",
    "\n",
    "* Weekend (F-M)\n",
    "\n",
    "* Peak times\n",
    "\n",
    "* Tailnum -> seats\n",
    "\n",
    "* Recent (previous) delays by carrier\n",
    "\n",
    "* Recent delays by origin/dest airport\n",
    "\n",
    "* Plane history\n",
    "\n",
    "* 6-9 PM most delays https://www.rd.com/article/avoid-delays-best-time-day-to-fly/\n",
    "\n",
    "* Daily flight count\n",
    "\n",
    "# Process\n",
    "\n",
    "* Categorize as delayed/not, aim to capture as many delays as possible (recall>accuracy), accuracy can be determined in next\n",
    "\n",
    "* Categorize delay severity based on prior categorized delays (regression?)\n",
    "\n",
    "* Find agreement between models\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert numeric response to categorical\n",
    "def categorize_delays(delays):\n",
    "    result = np.where(delays < 30, 'ontime',\n",
    "             np.where((delays >= 30) & (delays <= 120), 'minordelay',\n",
    "             np.where(delays > 120, 'majordelay', delays)))\n",
    "    return result\n",
    "\n",
    "\n",
    "#match columns of original dataset\n",
    "def match_cols(original, new):\n",
    "    original_cols = original.columns\n",
    "\n",
    "    for col in original_cols:\n",
    "        if col not in new.columns:\n",
    "            new[col] = 0\n",
    "\n",
    "    new = new[original_cols]\n",
    "    return new\n",
    "\n",
    "\n",
    "#returns x,y\n",
    "def get_data(path):\n",
    "    flights = pd.read_csv(path)\n",
    "    planes = pd.read_csv('planes.csv')\n",
    "    weather = pd.read_csv('weather.csv')\n",
    "    modeldata = pd.read_csv('flight_data_full.csv') #used for column matching\n",
    "\n",
    "    #impute weather\n",
    "    weather_orig = weather['origin']\n",
    "    weather = weather.drop(['wind_gust','origin','time_hour','year'],axis=1)\n",
    "    imputer = IterativeImputer(sample_posterior=True)\n",
    "    weather = pd.DataFrame(imputer.fit_transform(weather), columns=weather.columns)\n",
    "    weather.insert(0, 'origin', weather_orig)\n",
    "\n",
    "    #impute airplanes\n",
    "    planes = planes.drop('speed',axis=1)\n",
    "    year_by_model = planes.groupby('model')['year'].first()\n",
    "    planes['year'] = planes['year'].fillna(planes['model'].map(year_by_model)) #still some missing... use median\n",
    "    planes['year'] = planes['year'].fillna(planes['year'].median())\n",
    "\n",
    "\n",
    "    ##### New variables #####\n",
    "    #delay severity\n",
    "    flights['delay_severity'] = categorize_delays(flights['dep_delay'])\n",
    "\n",
    "    #existance of a delay\n",
    "    flights['is_delayed'] = np.where(flights['delay_severity'] == 'ontime', 0, 1)\n",
    "\n",
    "    #snowing category\n",
    "    weather['snowing'] = (weather['precip'] > 0) & (weather['temp'] <= 32).astype(int)\n",
    "\n",
    "    #day of week + weekend category (F-M)\n",
    "    flights['date'] = pd.to_datetime(flights[['year', 'month', 'day']])\n",
    "    flights['day_of_week'] = flights['date'].dt.day_name()\n",
    "    flights['is_weekend'] = flights['day_of_week'].isin(['Friday', 'Saturday', 'Sunday', 'Monday']).astype(int)\n",
    "\n",
    "    #peak dates (Thanksgiving (11/28), Christmas, Memorial Day (5/27), July Fourth, and Labor Day(9/2)) pm 5 days\n",
    "    peak_dates = pd.to_datetime(['2013-11-28', '2013-12-25', '2013-07-04', '2013-05-27', '2013-09-02'])\n",
    "\n",
    "    peak_weeks = pd.DataFrame() #get 5 days before/after\n",
    "    for date in peak_dates:\n",
    "        date_range = pd.date_range(start=date - pd.Timedelta(days=5), \n",
    "                                end=date + pd.Timedelta(days=5))\n",
    "        peak_weeks = pd.concat([peak_weeks, pd.DataFrame({'date': date_range})], ignore_index=True)\n",
    "        \n",
    "    flights['peak_week'] = flights['date'].isin(peak_weeks['date']).astype(int)\n",
    "\n",
    "    #peak times (6PM-9PM)\n",
    "    flights['peak_time'] = flights['hour'].between(18, 21)\n",
    "    flights['peak_time'] = flights['peak_time'].astype(int)\n",
    "\n",
    "    #prior airline, origin, and destination delays (takes 2 min to run)\n",
    "    print('Getting new variables (1/3)')\n",
    "    flights['date'] = pd.to_datetime(flights[['year', 'month', 'day', 'hour', 'minute']])\n",
    "\n",
    "    flights['carrier_delay'] = flights.apply(\n",
    "        lambda row: flights[(flights['carrier'] == row['carrier']) & \n",
    "                            (flights['date'] <= row['date']) & \n",
    "                            (flights['date'] > row['date'] - pd.Timedelta(hours=48))]['dep_delay'].mean(), axis=1)\n",
    "\n",
    "    print('Getting new variables (2/3)')\n",
    "    flights['origin_delay'] = flights.apply(\n",
    "        lambda row: flights[(flights['origin'] == row['origin']) & \n",
    "                            (flights['date'] <= row['date']) & \n",
    "                            (flights['date'] > row['date'] - pd.Timedelta(hours=48))]['dep_delay'].mean(), axis=1)\n",
    "\n",
    "    print('Getting new variables (3/3)')\n",
    "    flights['dest_delay'] = flights.apply(\n",
    "        lambda row: flights[(flights['dest'] == row['dest']) & \n",
    "                            (flights['date'] <= row['date']) & \n",
    "                            (flights['date'] > row['date'] - pd.Timedelta(hours=48))]['dep_delay'].mean(), axis=1)\n",
    "\n",
    "    flights['carrier_delay'] = categorize_delays(flights['carrier_delay'])\n",
    "    flights['carrier_delay'] = np.where(flights['carrier_delay'] == 'ontime', 0, 1)\n",
    "\n",
    "    flights['origin_delay'] = categorize_delays(flights['origin_delay'])\n",
    "    flights['origin_delay'] = np.where(flights['origin_delay'] == 'ontime', 0, 1)\n",
    "\n",
    "    flights['dest_delay'] = categorize_delays(flights['dest_delay'])\n",
    "    flights['dest_delay'] = np.where(flights['dest_delay'] == 'ontime', 0, 1)\n",
    "\n",
    "    #number of flights leaving airport same day\n",
    "    flights['flight_volume'] = flights.apply(\n",
    "        lambda row: len(flights[(flights['origin'] == row['origin']) & \n",
    "                            (flights['year'] == row['year']) & \n",
    "                            (flights['month'] == row['month']) & \n",
    "                            (flights['day'] == row['day'])]),axis=1)\n",
    "\n",
    "    #create final dataset\n",
    "    flights = pd.merge(flights, weather, on=['month', 'day', 'hour', 'origin'])\n",
    "\n",
    "    planes['year_manufactured'] = planes['year']\n",
    "    planes = planes.drop('year',axis=1)\n",
    "    flights = pd.merge(flights, planes, on='tailnum')\n",
    "\n",
    "    #responses\n",
    "    ys = flights[['dep_delay', 'delay_severity', 'is_delayed']]\n",
    "\n",
    "    flights = flights.drop(['arr_time', 'arr_delay', 'flight','date','tailnum','air_time',\n",
    "                            'year', 'month', 'day', 'dest', 'dep_time', 'dep_delay', 'delay_severity', 'is_delayed'],axis=1)\n",
    "    \n",
    "    modeldata = modeldata.drop(['Unnamed: 0','air_time','year', 'month', 'day', 'dest', 'dep_time', 'dep_delay', 'delay_severity', 'is_delayed'],axis=1)\n",
    "\n",
    "    #predictors\n",
    "    x = pd.get_dummies(flights,dtype=int)\n",
    "    modeldata = pd.get_dummies(modeldata,dtype=int)\n",
    "\n",
    "    #match columns to original data\n",
    "    x = match_cols(modeldata, x)\n",
    "    \n",
    "    return x,ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting new variables (1/3)\n",
      "Getting new variables (2/3)\n",
      "Getting new variables (3/3)\n"
     ]
    }
   ],
   "source": [
    "##### DON'T RUN THIS #####\n",
    "x,ys = get_data('flights_set0.csv')\n",
    "y = ys['is_delayed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Data #####\n",
    "data = pd.read_csv('flight_data_full.csv')\n",
    "data = data.drop(['Unnamed: 0','air_time','year', 'month', 'day', 'dest', 'dep_time'],axis=1)\n",
    "\n",
    "\n",
    "#data for first model\n",
    "x = data.drop(['dep_delay', 'delay_severity', 'is_delayed'],axis=1)\n",
    "x = pd.get_dummies(x,dtype=int)\n",
    "y = data['is_delayed']\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(x,y,train_size=.7,random_state=764)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "* Kappa .457\n",
    "\n",
    "* .79 recall on both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'ccp_alpha': 0.0, 'max_depth': None, 'min_impurity_decrease': 0.1, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "dc = DecisionTreeClassifier(class_weight='balanced')\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [None, 2, 5, 7, 10],\n",
    "    'min_samples_split': [2, 3, 5, 10, 20],\n",
    "    'min_impurity_decrease': [0.0, 0.01, 0.1],\n",
    "    'ccp_alpha': [0.0, 0.1, 0.2, 0.5, 0.7, 1.0] \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=dc, param_grid=param_grid, cv=5, n_jobs=-1, verbose=0, scoring=cohen_kappa_scorer)\n",
    "\n",
    "grid_search.fit(x_train1, y_train1)\n",
    "best_tree = grid_search.best_estimator_\n",
    "y_pred = best_tree.predict(x_test1)\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameters: {'ccp_alpha': 0.0, 'max_depth': None, 'min_impurity_decrease': 0.1, 'min_samples_split': 2}\n",
    "\n",
    "rf = RandomForestClassifier(class_weight='balanced',ccp_alpha=0,max_depth=None,min_impurity_decrease=0.1,min_samples_split=2)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 70, 90, 100, 150, 200, 300],\n",
    "    'max_features': ['auto', 'sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=0, scoring=cohen_kappa_scorer)\n",
    "\n",
    "grid_search.fit(x_train1, y_train1)\n",
    "best_tree = grid_search.best_estimator_\n",
    "print(grid_search.best_score_)\n",
    "y_pred = best_tree.predict(x_test1)\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.93      5595\n",
      "           1       0.83      0.35      0.49      1083\n",
      "\n",
      "    accuracy                           0.88      6678\n",
      "   macro avg       0.86      0.67      0.71      6678\n",
      "weighted avg       0.88      0.88      0.86      6678\n",
      "\n",
      "Adjusted Threshold Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.78      0.86      5595\n",
      "           1       0.41      0.78      0.54      1083\n",
      "\n",
      "    accuracy                           0.78      6678\n",
      "   macro avg       0.68      0.78      0.70      6678\n",
      "weighted avg       0.86      0.78      0.80      6678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Best parameters: {'max_features': 'auto', 'n_estimators': 50}, kappa = .457\n",
    "\n",
    "rf = RandomForestClassifier(class_weight='balanced',ccp_alpha=0,max_depth=None,min_samples_split=2,\n",
    "                            max_features='sqrt',n_estimators=200)\n",
    "\n",
    "rf.fit(x_train1, y_train1)\n",
    "y_pred = rf.predict(x_test1)\n",
    "print(classification_report(y_test1, y_pred))\n",
    "\n",
    "y_pred_prob = rf.predict_proba(x_test1)[:, 1]\n",
    "threshold = 0.15 #.79 both\n",
    "y_pred_adjusted = (y_pred_prob >= threshold).astype(int)\n",
    "\n",
    "print(\"Adjusted Threshold Classification Report:\\n\", classification_report(y_test1, y_pred_adjusted))\n",
    "\n",
    "##### USE ADJUSTED #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "* Filter to only flights with severe delays for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2368, 15580]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 28\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# This is actually doing the cross-fold validation (which involves fitting our model for each split). \u001b[39;00m\n\u001b[1;32m     20\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m     21\u001b[0m     estimator\u001b[38;5;241m=\u001b[39melastic_net, \n\u001b[1;32m     22\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,  \u001b[38;5;66;03m# use all available processors\u001b[39;00m\n\u001b[1;32m     26\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train2)\n\u001b[1;32m     30\u001b[0m best_lr \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m     31\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:782\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_refit_for_multimetric(scorers)\n\u001b[1;32m    780\u001b[0m     refit_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefit\n\u001b[0;32m--> 782\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m    783\u001b[0m fit_params \u001b[38;5;241m=\u001b[39m _check_fit_params(X, fit_params)\n\u001b[1;32m    785\u001b[0m cv_orig \u001b[38;5;241m=\u001b[39m check_cv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv, y, classifier\u001b[38;5;241m=\u001b[39mis_classifier(estimator))\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:443\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \n\u001b[1;32m    426\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    442\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[0;32m--> 443\u001b[0m check_consistent_length(\u001b[38;5;241m*\u001b[39mresult)\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2368, 15580]"
     ]
    }
   ],
   "source": [
    "#scale and transform\n",
    "\n",
    "#filter to only delayed cases\n",
    "x_train_delayed = x_train[y_train1 == 1]\n",
    "x_test_delayed = x_test[y_test1 == 1]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(x_train_delayed)\n",
    "X_test = scaler.transform(x_test_delayed)\n",
    "\n",
    "#elastic net\n",
    "elastic_net = ElasticNet(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 1, 10, 100],  # sklearn likes to call lambda \"alpha\", it's bogus but we play along...\n",
    "    'l1_ratio': [0, 0.1, 0.3, 0.5, 0.7, 0.9, 1]  # sklearn likes to call alpha \"l1_ratio\", which is fine - just know this for your reference\n",
    "}\n",
    "\n",
    "# This is actually doing the cross-fold validation (which involves fitting our model for each split). \n",
    "grid_search = GridSearchCV(\n",
    "    estimator=elastic_net, \n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='neg_mean_squared_error',  # use RMSE\n",
    "    n_jobs=-1,  # use all available processors\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train2)\n",
    "\n",
    "best_lr = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.78494728168575\n",
      "44.94292410462379\n"
     ]
    }
   ],
   "source": [
    "y_baseline_pred = np.full_like(y_test2, np.median(y_train2))\n",
    "y_pred = best_lr.predict(X_test)\n",
    "\n",
    "best_model_rmse = np.sqrt(mean_squared_error(y_test2, y_pred))\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test2, y_baseline_pred))\n",
    "\n",
    "print(best_model_rmse) #better than baseline\n",
    "print(baseline_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  majordelay       0.00      0.00      0.00       210\n",
      "  minordelay       0.38      0.52      0.44       873\n",
      "      ontime       0.92      0.90      0.91      5595\n",
      "\n",
      "    accuracy                           0.83      6678\n",
      "   macro avg       0.43      0.47      0.45      6678\n",
      "weighted avg       0.82      0.83      0.82      6678\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred_adjusted = categorize_delays(y_pred)\n",
    "y_test3 = categorize_delays(y_test2)\n",
    "\n",
    "print(classification_report(y_test3, y_pred_adjusted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing data function on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting new variables (1/3)\n",
      "Getting new variables (2/3)\n",
      "Getting new variables (3/3)\n"
     ]
    }
   ],
   "source": [
    "x2,y2s = get_data('flights_set1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Threshold Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.89      0.94     11778\n",
      "           1       0.60      0.95      0.74      2125\n",
      "\n",
      "    accuracy                           0.90     13903\n",
      "   macro avg       0.80      0.92      0.84     13903\n",
      "weighted avg       0.93      0.90      0.91     13903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test new data\n",
    "y2 = y2s['is_delayed']\n",
    "\n",
    "y_pred = rf.predict(x2)\n",
    "y_pred_prob = rf.predict_proba(x2)[:, 1]\n",
    "threshold = 0.15 \n",
    "y_pred_adjusted = (y_pred_prob >= threshold).astype(int)\n",
    "\n",
    "print(\"Adjusted Threshold Classification Report:\\n\", classification_report(y2, y_pred_adjusted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
